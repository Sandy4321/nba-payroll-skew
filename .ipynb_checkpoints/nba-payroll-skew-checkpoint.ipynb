{
 "metadata": {
  "name": "",
  "signature": "sha256:1dd99b7e9ee3e7b3aa70fafc78690dbc65a5106781a94727034b6f0185731e6a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NBA Payroll Skewness\n",
      "====================\n",
      "*Analyzing the role of skewness in an NBA team's payroll in team performance*\n",
      "\n",
      "Introduction\n",
      "------------\n",
      "The NBA is known for having a complex salary system, with all sorts of restrictions and caps.  Moreover, star players often make waves as they sign multi-year, multi-million dollar contracts.  The following is a straightforward exercise that uses the `BeautifulSoup` Python package to scrape NBA payroll data from the web and apply analysis with respect to team performance.  \n",
      "  \n",
      "Teams\n",
      "----\n",
      "We will treat the payroll of a given team in a given year as one data point.  Although we could simply hardcode in the names of the NBA teams (go Warriors), we prefer to explore `bs4` a little bit.  To get the upcoming season's team names, we go to [ESPN's NBA Teams page](http://espn.go.com/nba/teams).  In order to leverage Beautiful Soup, we have to inspect the HTML of the page and see if there is a specific pattern for each div that contains a team name.  After doing so, we write the `is_team` method that checks BS tags, and make the request:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def is_team(tag):\n",
      "    return tag.has_attr('class') and tag.name == 'a' and ('bi' in tag['class'])\n",
      "\n",
      "url = 'http://espn.go.com/nba/teams'\n",
      "r = requests.get(url)\n",
      "soup = BeautifulSoup(r.text)\n",
      "teams = [team_tag.string for team_tag in soup.find_all(is_team)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print teams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Boston Celtics', u'Brooklyn Nets', u'New York Knicks', u'Philadelphia 76ers', u'Toronto Raptors', u'Golden State Warriors', u'Los Angeles Clippers', u'Los Angeles Lakers', u'Phoenix Suns', u'Sacramento Kings', u'Chicago Bulls', u'Cleveland Cavaliers', u'Detroit Pistons', u'Indiana Pacers', u'Milwaukee Bucks', u'Dallas Mavericks', u'Houston Rockets', u'Memphis Grizzlies', u'New Orleans Pelicans', u'San Antonio Spurs', u'Atlanta Hawks', u'Charlotte Hornets', u'Miami Heat', u'Orlando Magic', u'Washington Wizards', u'Denver Nuggets', u'Minnesota Timberwolves', u'Oklahoma City Thunder', u'Portland Trail Blazers', u'Utah Jazz']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can move to collect the payroll data for each team and for a range of years.  It appears that [Shamsports Data](data.shamsports.com) has the best collection that is publicly and freely available.  After exploring the website a bit, we hypothesize that a given team and year's payroll is available in the following URL format:  \n",
      "\"http://data.shamsports.com/content/pages/data/salaries/\" + year + \"/\" + team name + \".jsp\"  \n",
      "  \n",
      "For example, the 2012 Warriors (season = 2011-2012) payroll data is most easily accessible at \"http://data.shamsports.com/content/pages/data/salaries/2012/warriors.jsp\".  If you follow the link, you'll notice that that page also gives information for the 2012-2013 season, and the 2013-2014 season (though incomplete).  This is because the site attempts to project the payroll for future years given the then-current contracts.  To get the most accurate data, we'll want to use the `requests` package to get make separate requests for each data point.  \n",
      "  \n",
      "Let's check if our URL schema is right, and just how much data is available to us.  We do so with a 2d Numpy array of 0's and 1's, indicating failed and successful requests, respectively:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "base = 'http://data.shamsports.com/content/pages/data/salaries/'\n",
      "years = range(2000, 2015)\n",
      "x = np.ones((len(teams), len(years)), dtype=np.int)\n",
      "test_url = [t.split()[-1].lower() for t in teams]\n",
      "\n",
      "# if one of our rows is all 0s, that means the team url is actually wrong\n",
      "i = 0\n",
      "for tu in test_url:\n",
      "    j = 0\n",
      "    for year in years:\n",
      "        url = base + str(year) + '/' + tu + '.jsp'\n",
      "        r = requests.get(url)\n",
      "        if r.status_code == 404:\n",
      "            x[i, j] = 0 # mark it as a failed request\n",
      "            # print '404 for team \"' + tu + '\" and year ' + str(year)\n",
      "        j += 1\n",
      "    i += 1\n",
      "\n",
      "df = pd.DataFrame(x, index=test_url, columns=years)\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  \\\n",
        "celtics          0     0     0     0     0     0     0     1     1     1   \n",
        "nets             0     0     0     0     0     0     0     1     1     1   \n",
        "knicks           0     0     0     0     0     0     0     1     1     1   \n",
        "76ers            0     0     0     0     0     0     0     0     0     0   \n",
        "raptors          0     0     0     0     0     0     0     1     1     1   \n",
        "warriors         0     0     0     0     0     0     0     1     1     1   \n",
        "clippers         0     0     0     0     0     0     0     1     1     1   \n",
        "lakers           0     0     0     0     0     0     0     1     1     1   \n",
        "suns             0     0     0     0     0     0     0     1     1     1   \n",
        "kings            0     0     0     0     0     0     0     1     1     1   \n",
        "bulls            0     0     0     0     0     0     0     1     1     1   \n",
        "cavaliers        0     0     0     0     0     0     0     1     1     1   \n",
        "pistons          0     0     0     0     0     0     0     1     1     1   \n",
        "pacers           0     0     0     0     0     0     0     1     1     1   \n",
        "bucks            0     0     0     0     0     0     0     1     1     1   \n",
        "mavericks        0     0     0     0     0     0     0     1     1     1   \n",
        "rockets          0     0     0     0     0     0     0     1     1     1   \n",
        "grizzlies        0     0     0     0     0     0     0     1     1     1   \n",
        "pelicans         0     0     0     0     0     0     0     0     0     0   \n",
        "spurs            0     0     0     0     0     0     0     1     1     1   \n",
        "hawks            0     0     0     0     0     0     0     1     1     1   \n",
        "hornets          0     0     0     0     0     0     0     1     1     1   \n",
        "heat             0     0     0     0     0     0     0     1     1     1   \n",
        "magic            0     0     0     0     0     0     0     1     1     1   \n",
        "wizards          0     0     0     0     0     0     0     1     1     1   \n",
        "nuggets          0     0     0     0     0     0     0     1     1     1   \n",
        "timberwolves     0     0     0     0     0     0     0     1     1     1   \n",
        "thunder          0     0     0     0     0     0     0     0     0     1   \n",
        "blazers          0     0     0     0     0     0     0     1     1     1   \n",
        "jazz             0     0     0     0     0     0     0     1     1     1   \n",
        "\n",
        "              2010  2011  2012  2013  2014  \n",
        "celtics          1     1     1     1     1  \n",
        "nets             1     1     1     1     1  \n",
        "knicks           1     1     1     1     1  \n",
        "76ers            0     0     0     0     0  \n",
        "raptors          1     1     1     1     1  \n",
        "warriors         1     1     1     1     1  \n",
        "clippers         1     1     1     1     1  \n",
        "lakers           1     1     1     1     1  \n",
        "suns             1     1     1     1     1  \n",
        "kings            1     1     1     1     1  \n",
        "bulls            1     1     1     1     1  \n",
        "cavaliers        1     1     1     1     1  \n",
        "pistons          1     1     1     1     1  \n",
        "pacers           1     1     1     1     1  \n",
        "bucks            1     1     1     1     1  \n",
        "mavericks        1     1     1     1     1  \n",
        "rockets          1     1     1     1     1  \n",
        "grizzlies        1     1     1     1     1  \n",
        "pelicans         0     0     0     1     1  \n",
        "spurs            1     1     1     1     1  \n",
        "hawks            1     1     1     1     1  \n",
        "hornets          1     1     1     0     1  \n",
        "heat             1     1     1     1     1  \n",
        "magic            1     1     1     1     1  \n",
        "wizards          1     1     1     1     1  \n",
        "nuggets          1     1     1     1     1  \n",
        "timberwolves     1     1     1     1     1  \n",
        "thunder          1     1     1     1     1  \n",
        "blazers          1     1     1     1     1  \n",
        "jazz             1     1     1     1     1  \n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we check which rows are all 0 (which URL's are completely incorrect), we find the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print df[(df.T == 0).all()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  \\\n",
        "76ers     0     0     0     0     0     0     0     0     0     0     0     0   \n",
        "\n",
        "       2012  2013  2014  \n",
        "76ers     0     0     0  \n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like 76ers is just wrong (404 for all years attempted), so let's try the URL 'sixers'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tu = u'sixers'\n",
      "i = 3 ## << index of the sixers\n",
      "j = 0\n",
      "for year in years:\n",
      "    url = base + str(year) + '/' + tu + '.jsp'\n",
      "    r = requests.get(url)\n",
      "    if r.status_code != 404:\n",
      "        x[i, j] = 1 # score (jk Sixers don't know how to do that)\n",
      "        # print 'URL valid for team \"' + tu + '\" and year ' + str(year)\n",
      "    j += 1\n",
      "\n",
      "test_url[3] = tu\n",
      "df = pd.DataFrame(x, index=test_url, columns=years)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we can look at which columns are all zero, which can let us know the earliest season that the website has data for:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print df.columns[(df == 0).all()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Int64Index([2000, 2001, 2002, 2003, 2004, 2005, 2006], dtype='int64')\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the earliest season is 2007, then, we slice our Pandas DataFrame and `pickle` it for further usage:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df.loc[:, ~(df == 0).all()]\n",
      "\n",
      "import pickle\n",
      "pickle.dump(df, open('valid_pages.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also note that there are 0s for valid URL's past 2007 -- this is due to the unfortunate franchise relocations and renamings (New Orleans Hornets became the New Orleans Pelicans, etc.).  We'll proceed with our data frame of valid pages, scrape the salaries, and fill in the non-standard holes afterwards."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Salaries\n",
      "--------\n",
      "Now that we have our valid pages, we begin the iterative process of scraping the actual salary numbers from each valid page.  We simply leave here the final result (which includes hard-coding some teams where the HTML is just too messy.  Honestly, what can you do when the website is designed by someone who includes \"$19,000,000ish\" as a data point, or decides to randomly not add a new tr tag but still count it as a row?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_salary_table(tag):\n",
      "    return tag.name == 'table' and (tag.tr is not None) and \\\n",
      "    (all([(t.has_attr('class') and t['class'][0] == 'blue4') for \n",
      "        t in tag.tr.findAll('td')]))\n",
      "def is_total_row(tag):\n",
      "    return tag.name == 'tr' and (tag.td is not None) and (tag.td.string is not None) \\\n",
      "    and (re.sub(':', '', ''.join(tag.td.string.split()).lower()) == 'totalsalaries') \n",
      "    # classic \"Total salaries\" vs \"Total Salaries\" vs \"Total Salaries:\"\n",
      "def is_player_row(tag):\n",
      "    return tag.name == 'tr' and (tag.td is not None) and (tag.td.find('a') is not None) and \\\n",
      "    (len(tag.findAll('td')) >= 6)\n",
      "def float_conv(s):\n",
      "    if len(s) == 0:\n",
      "        return 0.\n",
      "    return float(s)\n",
      "\n",
      "# given the url to the shamsports.com site\n",
      "def extractSalaryPercents(url):\n",
      "    r = requests.get(url)\n",
      "    soup = BeautifulSoup(r.text)\n",
      "    if url == 'http://data.shamsports.com/content/pages/data/salaries/2008/mavericks.jsp':\n",
      "        tot = 106948621.\n",
      "    elif url == 'http://data.shamsports.com/content/pages/data/salaries/2007/jazz.jsp':\n",
      "        tot = 61310247.\n",
      "    else:\n",
      "        total_row = soup.find(is_total_row)\n",
      "        totals = [float(re.sub(',', '', t.string[1:])) for t in total_row.findAll('b')[1:] if (t.string is not None)]\n",
      "        tot = totals[0]\n",
      "\n",
      "    sal_table = soup.findAll(is_salary_table)[0]\n",
      "    player_rows = sal_table.findAll(is_player_row)\n",
      "    sal_strings = [p.findAll('td')[1].string for p in player_rows]\n",
      "    sal_strings = [s for s in sal_strings if s != 'N/A']\n",
      "    sal_strings = [re.sub(r'[a-zA-Z]', '', s) for s in sal_strings]\n",
      "    sals = [float_conv(re.sub(',', '', re.sub('\\$', '', s))) for s in sal_strings]\n",
      "    sal_percs = [sal / tot for sal in sals]\n",
      "    return sal_percs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're going to track the salary in another Pandas DataFrame -- the number of columns is 42 simply because we want to allow for strange circumstances where teams are paying more than ~12 players.  We omit the output from the code below because, as explained above, it was an iterative process that resulted in many adjustments to the functions defined above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pickle.load(open('valid_pages.p', 'rb'))\n",
      "base = 'http://data.shamsports.com/content/pages/data/salaries/'\n",
      "###################\n",
      "# columns: team | year | player 1 salary percentage | player 2 salary percentage ...\n",
      "ncol = 42\n",
      "sal_perc_df = pd.DataFrame(np.ones((df.values[df.values == 1].size, ncol), dtype=np.float64), \n",
      "                           columns=['team', 'year'] + ['p' + str(i) for i in range(1,41)])\n",
      "rc = 0\n",
      "\n",
      "# fill the salary df\n",
      "scrape_bad = False\n",
      "for i in range(0, df.shape[0]):\n",
      "    for j in range(0, df.shape[1]):\n",
      "        if df.iloc[i, j] == 1:\n",
      "            sal_perc_df['team'][rc] = df.index[i]\n",
      "            sal_perc_df['year'][rc] = df.columns[j]\n",
      "            # scrape salary data\n",
      "            url = base + str(df.columns[j]) + '/' + df.index[i] + '.jsp'\n",
      "            try:\n",
      "                sal_percs = cs.extractSalaryPercents(url)\n",
      "            except:\n",
      "                print 'Scraping error on url <' + url + '>'\n",
      "                print 'Improve your extract function, you noob.'\n",
      "                scrape_bad = True\n",
      "                break\n",
      "            sal_percs += [0] * ((ncol - 2) - len(sal_percs)) \n",
      "            # pad the zeros\n",
      "            sal_perc_df.iloc[rc, 2:] = sal_percs # fill that row in         \n",
      "            rc += 1\n",
      "            print 'Finished payroll for ' + df.index[i] + ', ' + str(df.columns[j])\n",
      "        else:\n",
      "            print 'Skipping 404 page'\n",
      "    if scrape_bad:\n",
      "        break   \n",
      "\n",
      "pickle.dump(sal_perc_df, open('payroll_shamsports.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Filling in the Holes\n",
      "--------------------\n",
      "Our salary data frame is looking pretty good:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sal_perc_df = pickle.load(open('payroll_shamsports.p', 'rb'))\n",
      "print sal_perc_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "      team  year        p1        p2        p3        p4        p5        p6  \\\n",
        "0  celtics  2007  0.241152  0.188031  0.186301  0.083729  0.044602  0.033398   \n",
        "1  celtics  2008  0.319724  0.220223  0.215375  0.060317  0.043156  0.040383   \n",
        "2  celtics  2009  0.312568  0.232209  0.228288  0.057822  0.040497  0.033464   \n",
        "3  celtics  2010  0.233356  0.221346  0.193528  0.069008  0.058941  0.053047   \n",
        "4  celtics  2011  0.247299  0.182222  0.131318  0.119380  0.075705  0.072791   \n",
        "\n",
        "         p7        p8    ...     p31  p32  p33  p34  p35  p36  p37  p38  p39  \\\n",
        "0  0.030961  0.028634    ...       0    0    0    0    0    0    0    0    0   \n",
        "1  0.025147  0.020191    ...       0    0    0    0    0    0    0    0    0   \n",
        "2  0.031570  0.016607    ...       0    0    0    0    0    0    0    0    0   \n",
        "3  0.040243  0.035365    ...       0    0    0    0    0    0    0    0    0   \n",
        "4  0.058515  0.039396    ...       0    0    0    0    0    0    0    0    0   \n",
        "\n",
        "   p40  \n",
        "0    0  \n",
        "1    0  \n",
        "2    0  \n",
        "3    0  \n",
        "4    0  \n",
        "\n",
        "[5 rows x 42 columns]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "but again, there are holes in the rows where we had \"invalid\" pages in our ShamSports URL testing.  What we really want is to rename all the teams by their city and year (so we don't run into problems with Charlotte Hornets vs. Charlotte Bobcats vs. New Orleans Hornets), and then individually scrape the missing data.  We do so with the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from the team history page for each of those team names (team names)\n",
      "# for the upcoming \"2015\" season, we have\n",
      "#\n",
      "# hornets: \n",
      "# 2015 - present    NBA     Charlotte Hornets\n",
      "# 2005 - 2014       NBA     Charlotte Bobcats\n",
      "# 1989 - 2002       NBA     Charlotte Hornets\n",
      "# \n",
      "# pelicans:\n",
      "# 2014 - present    NBA     New Orleans Pelicans\n",
      "# 2007 - 2013       NBA     New Orleans Hornets\n",
      "# 2006 - 2007       NBA     New Orleans/Oklahoma City Hornets\n",
      "# 2003 - 2006       NBA     New Orleans Hornets\n",
      "#\n",
      "# thunder:\n",
      "# 2009 - present    NBA     Oklahoma City Thunder\n",
      "# 1968 - 2008       NBA     Seattle Supersonics\n",
      "\n",
      "# this means that we need to retrieve the following salary data to make \n",
      "# our data frame 30 (number of teams) * 8 (number of seasons we have) \n",
      "# = 240 rows, and also label the team names as 'Charlotte Hornets'\n",
      "# instead of just 'hornets' so our data frame will look more like:\n",
      "# new_york_knicks         2007    x0\n",
      "# new_orleans_pelicans    2014    x1\n",
      "# new_orleans_hornets     2013    x2, etc.\n",
      "\n",
      "# names of current teams\n",
      "teams = ['_'.join(t.lower().split()) for t in get_teams.retrieve()]\n",
      "\n",
      "ncol = 42\n",
      "sal_perc_df_full = pd.DataFrame(np.ones((240, ncol), \n",
      "    dtype=np.float64), columns=['team', 'year'] + ['p' + str(i) for i in range(1,ncol - 1)]) \n",
      "\n",
      "num_seasons = 8 # 2007 - 2014\n",
      "old_count = 0 # tracks where we are in sal_perc_df\n",
      "for i in range(0, len(teams)):\n",
      "\n",
      "    inds = range((i * num_seasons),((i + 1) * num_seasons))\n",
      "\n",
      "    if sal_perc_df['team'][old_count] == 'hornets':\n",
      "        sal_perc_df_full['team'][inds] = u'charlotte_bobcats'\n",
      "        sal_perc_df_full['year'][inds] = range(2007, 2015)\n",
      "        old_count += sal_perc_df[sal_perc_df['team'] == 'hornets'].shape[0]\n",
      "\n",
      "    elif sal_perc_df['team'][old_count] == 'pelicans':\n",
      "        sal_perc_df_full['team'][inds[0:-1]] = u'new_orleans_hornets'\n",
      "        sal_perc_df_full['team'][inds[-1]] = u'new_orleans_pelicans'\n",
      "        sal_perc_df_full['year'][inds] = range(2007, 2015)\n",
      "        old_count += sal_perc_df[sal_perc_df['team'] == 'pelicans'].shape[0]\n",
      "    elif sal_perc_df['team'][old_count] == 'thunder':\n",
      "        sal_perc_df_full['team'][inds[0:2]] == u'seattle_supersonics'\n",
      "        sal_perc_df_full['team'][inds[2:]] == u'oklahoma_city_thunder'\n",
      "        sal_perc_df_full['year'][inds] = range(2007, 2015)\n",
      "        old_count += sal_perc_df[sal_perc_df['team'] == 'thunder'].shape[0]\n",
      "    else:\n",
      "        # name the team the right name\n",
      "        sal_perc_df_full['team'][inds] = teams[i]\n",
      "        sal_perc_df_full['year'][inds] = range(2007, 2015)\n",
      "        old_percs = sal_perc_df.iloc[old_count:old_count + num_seasons,\n",
      "            2:ncol]\n",
      "        sal_perc_df_full.iloc[inds, 2:ncol] = old_percs\n",
      "        old_count = old_count + num_seasons\n",
      "\n",
      "# now we need to fill in salary data for Charlotte Bobcats,\n",
      "# New Orleans Hornets/New Orleans Pelicans, Seattle Supersonics/\n",
      "# Oklahoma City Thunder\n",
      "\n",
      "# NO Hornets\n",
      "sal_perc_df_full.iloc[144:150, 2:] = sal_perc_df[sal_perc_df['team'] == 'hornets'].iloc[0:-1, 2:]\n",
      "# NO Pelicans\n",
      "sal_perc_df_full.iloc[150:152, 2:] = sal_perc_df[sal_perc_df['team'] == 'pelicans'].iloc[:, 2:]\n",
      "# OKC\n",
      "sal_perc_df_full.iloc[218:224, 2:] = sal_perc_df[sal_perc_df['team'] == 'thunder'].iloc[:, 2:]\n",
      "\n",
      "# now salaries that we haven't collected:\n",
      "# Supersonics data can't be found on ShamSports, so we're making it nan\n",
      "sal_perc_df_full.iloc[216:218, 2:] = float('nan')\n",
      "\n",
      "# Charlotte Bobcats (only changing to Charlotte Hornets for \n",
      "# 2014-2015 season)\n",
      "sal_perc_df_full.iloc[168:176, 0:3]\n",
      "\n",
      "i = 0\n",
      "for year in range(2007, 2015):\n",
      "    sal_percs = cs.extractSalaryPercents('http://data.shamsports.com/content/pages/data/salaries/' + str(year) + '/bobcats.jsp')\n",
      "    sal_percs += [0] * ((ncol - 2) - len(sal_percs))\n",
      "    sal_perc_df_full.iloc[168 + i, 2:] = sal_percs\n",
      "    i += 1\n",
      "\n",
      "pickle.dump(sal_perc_df_full, open('fixed_payroll_shamsports.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculating Skew\n",
      "--------------------\n",
      "Now that we've filled all of the holes in our salary data, we can start calculating the skews from the payroll.  Since we already have them in proportions, we want to first check that they roughly sum to 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pickle\n",
      "\n",
      "sal_perc_df = pickle.load(open('fixed_payroll_shamsports.p', 'rb'))\n",
      "\n",
      "# check that each row sums (approximately) to 1 to see that we didn't mess up a ton\n",
      "better_equal_one = sal_perc_df.iloc[:, 2:].sum(axis=1)\n",
      "print sorted(better_equal_one[better_equal_one != 1])\n",
      "print better_equal_one[better_equal_one - 1 > 0.5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.96527300177169195, 0.98492868220182306, 0.98574455796005434, 0.98947687117493133, 0.99892669169006287, 0.99973853494540144, 0.99981418282636414, 0.9999653597276813, 0.99997115633544542, 0.99999455246515967, 0.99999976565559301, 0.99999994768642708, 0.99999997181162081, 0.99999998138785484, 0.99999998395456446, 0.9999999841251519, 0.9999999847093024, 0.99999998818358604, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 0.99999999999999989, 1.0000000000000002, 1.0000000112953704, 1.0000000151036756, 1.0000002228311256, 1.0000008957968189, 1.0000009229498401, 1.0000013666456631, 1.0000015897024968, 1.0000046873976838, 1.0000047862391415, 1.0000053474104555, 1.0000114974089711, 1.0000263507025786, 1.0001134220037364, 1.0040429990635693, 1.0070373870797189, 1.0165472899303316, 1.0181751820420755, 1000.0, nan, nan]\n",
        "202    1000\n",
        "dtype: float64\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like we have one team with a sum of 1000 at index 202, as well as two nan values at the end (unavailable salary data)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sal_perc_df.iloc[202,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "team    denver_nuggets\n",
        "year              2009\n",
        "p1            206.4831\n",
        "p2            206.1792\n",
        "p3            158.0978\n",
        "p4            138.4965\n",
        "p5            71.32285\n",
        "p6            55.26411\n",
        "p7            46.64192\n",
        "p8              27.755\n",
        "p9            26.10392\n",
        "p10           19.60012\n",
        "p11           11.41138\n",
        "p12           11.41138\n",
        "p13           11.41138\n",
        "p14           6.325542\n",
        "p15            2.75216\n",
        "p16          0.7436461\n",
        "p17                  0\n",
        "p18                  0\n",
        "p19                  0\n",
        "p20                  0\n",
        "p21                  0\n",
        "p22                  0\n",
        "p23                  0\n",
        "p24                  0\n",
        "p25                  0\n",
        "p26                  0\n",
        "p27                  0\n",
        "p28                  0\n",
        "p29                  0\n",
        "p30                  0\n",
        "p31                  0\n",
        "p32                  0\n",
        "p33                  0\n",
        "p34                  0\n",
        "p35                  0\n",
        "p36                  0\n",
        "p37                  0\n",
        "p38                  0\n",
        "p39                  0\n",
        "p40                  0\n",
        "Name: 202, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like nuggets 2009 was a mess up that didn't give an error -- after examining the actual page via browser, we see that due to more inconsistencies in the data display, all the percents are multiplied by 1000, which is easily resolved."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sal_perc_df.iloc[202,2:] = sal_perc_df.iloc[202,2:] / 1000."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We leave the two nan's to be left out during our calculations, and proceed to calculate the skew for each team.  This is done with a simple, intuitive formula inspired by LAD regression: for a given team, we take the \"equal salary\" proportion as 1 divided by the number of paid players, and sum over the absolute deviations for each individual salary proportion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# skew array to be filled\n",
      "skews = np.zeros(shape=(sal_perc_df.shape[0], 1))\n",
      "for i in range(0, sal_perc_df.shape[0]):\n",
      "    # check for the supersonics ones\n",
      "    if np.isnan(sal_perc_df.iloc[i, 2]):\n",
      "        skews[i] = float('nan')\n",
      "    else:\n",
      "        # subtract two for the team name and year\n",
      "        n = float(sal_perc_df[i:i+1][sal_perc_df[i:i+1] != 0].transpose().count() - 2)\n",
      "        eq = 1 / n\n",
      "        these_sal = sal_perc_df.iloc[i, 2:]\n",
      "\n",
      "        # filter out the zeroes before calculating starpower\n",
      "        sp = sum(abs(these_sal[these_sal != 0] - eq))\n",
      "        skews[i] = float(sp)\n",
      "print skews[0:10]\n",
      "pickle.dump(skews, open('skews.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.97736017]\n",
        " [ 1.11909633]\n",
        " [ 1.12795592]\n",
        " [ 0.93447634]\n",
        " [ 0.95154974]\n",
        " [ 1.0255616 ]\n",
        " [ 1.23490221]\n",
        " [ 0.97730874]\n",
        " [ 1.08862991]\n",
        " [ 0.87340053]]\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's clear that a skew (as defined here) that is 0 has perfect equality, and skewness increases in the other direction with the value."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Team Performances\n",
      "-----------------\n",
      "In order to evaluate the impact of payroll skew, we also need to scrape the performances of each team (in each year).  We use LandOfBasketball this time, which is slightly easier to scrape from (although, as there always is, there are some holes that need to be fixed).  Since the data is relatively easily available, we extract both regular season and playoff performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import requests\n",
      "import re\n",
      "import pickle\n",
      "from bs4 import BeautifulSoup\n",
      "from sort_helper import uniquify\n",
      "\n",
      "import get_teams\n",
      "\n",
      "# load in the salary df just to pull the teams/years\n",
      "sal_perc_df = pickle.load(open('fixed_payroll_shamsports.p', 'rb'))\n",
      "\n",
      "performance_df = pd.DataFrame(np.ones((sal_perc_df.shape[0], 5), \n",
      "    dtype=np.float64), columns=['team', 'year', 'skew',\n",
      "    'reg_season_wp', 'playoff_wp'])\n",
      "\n",
      "# the year is (as per ShamSports) the ending year of the season,\n",
      "# so the 2013-2014 season is under 2014\n",
      "performance_df['team'] = sal_perc_df['team']\n",
      "performance_df['year'] = sal_perc_df['year']\n",
      "performance_df['skew'] = pickle.load(open('skews.p', 'rb'))\n",
      "\n",
      "# the URLs to each team's info page on landofbasketball \n",
      "# will be alphabetical order -- let's sort our df as well.\n",
      "# because the landofbasketball links will give data for each\n",
      "# franchise, we need to order by current team, so we can do the\n",
      "# following:\n",
      "\n",
      "performance_df.team[performance_df.team == 'charlotte_bobcats'] = u'charlotte_hornets_charlotte_bobcats'\n",
      "performance_df.team[performance_df.team == 'seattle_supersonics'] = u'oklahoma_city_thunder_seattle_supersonics'\n",
      "performance_df.team[performance_df.team == 'oklahoma_city_thunder'] = u'oklahoma_city_thunder_z'\n",
      "performance_df.team[performance_df.team == 'new_orleans_hornets'] = u'new_orleans_pelicans_new_orleans_hornets'\n",
      "performance_df.team[performance_df.team == 'new_orleans_pelicans'] = u'new_orleans_pelicans_z'\n",
      "\n",
      "s_perf_df = performance_df.sort(['team', 'year'], ascending=[True, False])\n",
      "\n",
      "base = 'http://www.landofbasketball.com/'\n",
      "url = base + 'nba_teams.htm'\n",
      "r = requests.get(url)\n",
      "soup = BeautifulSoup(r.text)\n",
      "\n",
      "def isTeamURL(tag):\n",
      "    return tag.name == 'a' and tag.string is not None and ''.join(tag.string.lower().split()) == 'yearlyrecords'\n",
      "\n",
      "team_urls = [base + tag['href'] for tag in soup.findAll(isTeamURL)]\n",
      "\n",
      "\n",
      "def isSeasonRow(tag):\n",
      "    return tag.name == 'tr' and tag.td is not None and \\\n",
      "    tag.td.a is not None and tag.td.a.string is not None and \\\n",
      "    (re.search('[0-9]{4}-[0-9]{2}', tag.td.a.string) is not None)\n",
      "\n",
      "# helper for finding the records after the season row\n",
      "def isTd(tag):\n",
      "    return tag.name == 'tr'\n",
      "\n",
      "counter = 0\n",
      "for url in team_urls:\n",
      "    name_end = re.search('.htm', url).start()\n",
      "    full_team = url[re.search('_', url).start() + 1:-4]\n",
      "    r = requests.get(url)\n",
      "    soup = BeautifulSoup(r.text)\n",
      "\n",
      "    # 0 index is the 2014-2015 season\n",
      "    percs = soup.findAll(isSeasonRow)[1:9]\n",
      "    for p in percs:\n",
      "        tds = p.findAll('td')\n",
      "        reg = float(tds[3].string)\n",
      "        playoff = tds[7].string\n",
      "        if playoff == '-':\n",
      "            playoff = float('nan')\n",
      "        s_perf_df.iat[counter, 3] = reg\n",
      "        s_perf_df.iat[counter, 4] = playoff\n",
      "        counter += 1\n",
      "\n",
      "# rename, and then set the name as index instead of columns\n",
      "s_perf_df.team[s_perf_df.team == 'charlotte_hornets_charlotte_bobcats'] = u'charlotte_bobcats'\n",
      "s_perf_df.team[s_perf_df.team == 'oklahoma_city_thunder_seattle_supersonics'] = u'seattle_supersonics'\n",
      "s_perf_df.team[s_perf_df.team == 'oklahoma_city_thunder_z'] = u'oklahoma_city_thunder'\n",
      "s_perf_df.team[s_perf_df.team == 'new_orleans_pelicans_new_orleans_hornets'] = u'new_orleans_hornets'\n",
      "s_perf_df.team[s_perf_df.team == 'new_orleans_pelicans_z'] = u'new_orleans_pelicans'\n",
      "\n",
      "pickle.dump(s_perf_df, open('skew_record.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(s_perf_df.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              team  year      skew  reg_season_wp  playoff_wp\n",
        "167  atlanta_hawks  2014  1.086202          0.463       0.429\n",
        "166  atlanta_hawks  2013  0.941220          0.537       0.333\n",
        "165  atlanta_hawks  2012  1.112279          0.606       0.333\n",
        "164  atlanta_hawks  2011  0.955714          0.537       0.500\n",
        "163  atlanta_hawks  2010  0.905268          0.646       0.364\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Writing this to a `.csv` file so we can use `R-ggplot2` instead of crappy `matplotlib`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s_perf_df.to_csv(\"skew_record.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The `R` code to produce the plots and correlation tests are in the <a href=\"https://github.com/edz504/nba-payroll-skew\" target=\"blank\"> associated Git repo </a>.  Correlation tests (Pearson, Kendall Tau, and Spearman) were run for all available data for skew/regular season performance, skew/playoff win performance, and (just for fun) regular season performance/playoff win performance."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}